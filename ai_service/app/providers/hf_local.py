from __future__ import annotations

import os
from typing import Any, List

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from app.providers.base import LLMProvider


class HFLocalProvider(LLMProvider):
    """Hugging Face local/hosted model provider using transformers pipeline."""

    def __init__(self, model_id: str, auth_token: str | None = None, device: int | str | None = None):
        self.model_id = model_id
        self.auth_token = auth_token or os.getenv("HF_TOKEN")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            token=self.auth_token,
            trust_remote_code=True,
            use_fast=False,
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            token=self.auth_token,
            trust_remote_code=True,
        )
        # Ensure eager attention on CPU to avoid flash-attn warnings
        try:
            self.model.config.attn_implementation = "eager"
        except Exception:
            pass
        # Disable cache to avoid DynamicCache seen_tokens issues on some model/torch combos
        try:
            self.model.config.use_cache = False
        except Exception:
            pass

        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            device=device,
            torch_dtype=torch.float32,
        )

    async def chat(self, messages: list[dict[str, str]], tenant_id: str, temperature: float = 0.7) -> str:
        prompt = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
        result = self.generator(
            prompt,
            max_new_tokens=128,
            do_sample=True,
            temperature=temperature,
            use_cache=False,
        )
        return result[0]["generated_text"][-512:]

    async def propose_actions(self, messages: list[dict[str, str]], tenant_id: str) -> list[dict[str, Any]]:
        # Simple heuristic: return a placeholder until a structured prompt/parse is added
        return [{"type": "draft_email", "payload": {"body": "Generated by HF model", "tenant_id": tenant_id}}]

    async def embed(self, texts: list[str], tenant_id: str) -> list[list[float]]:
        # For embeddings, youâ€™d normally load a sentence-transformer model; this is a stub.
        return [[float(len(t))] for t in texts]

